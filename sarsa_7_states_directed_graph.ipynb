{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6ade691",
   "metadata": {},
   "source": [
    "![Graph](./image/seven_states_directed_graph.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4fa70dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SARSA implementation for a 7-state directed graph\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ebe73da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the reward matrix R\n",
    "R = np.array([\n",
    "    [-1, -1, -1,  0, -1, -1, -1],\n",
    "    [-1, -1,  0, -1, -1, -1, -1],\n",
    "    [-1,  0, -1,  0, -1,  0, -1],\n",
    "    [ 0, -1,  0, -1,  0, -1, -1],\n",
    "    [-1, -1, -1,  0, -1,  0, 100],\n",
    "    [-1, -1,  0, -1,  0, -1, 100],\n",
    "    [-1, -1, -1, -1, -1, -1, 100],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3f535a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = R.shape[0]\n",
    "Q = np.zeros_like(R, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "71ca5386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "alpha = 0.9      # Learning rate\n",
    "gamma = 0.8      # Discount factor\n",
    "epsilon = 0.1    # Epsilon for epsilon-greedy policy\n",
    "episodes = 5000  # Number of training episodes, sarsa is slow to converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "79232244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epsilon-greedy action selection\n",
    "\n",
    "def choose_action(state):\n",
    "    valid_actions = [a for a in range(n_states) if R[state, a] >= 0]\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice(valid_actions)\n",
    "    else:\n",
    "        q_vals = Q[state]\n",
    "        max_q = np.max([q_vals[a] if a in valid_actions else -np.inf for a in range(n_states)])\n",
    "        best_actions = [a for a in valid_actions if q_vals[a] == max_q]\n",
    "        return random.choice(best_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7c7c27e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SARSA training loop\n",
    "for _ in range(episodes):\n",
    "    state = random.randint(0, n_states - 1)\n",
    "    action = choose_action(state)\n",
    "\n",
    "    while state != 6:\n",
    "        next_state = action\n",
    "        next_action = choose_action(next_state)\n",
    "\n",
    "        # SARSA update rule\n",
    "        Q[state, action] += alpha * (\n",
    "            R[state, action] + gamma * Q[next_state, next_action] - Q[state, action]\n",
    "        )\n",
    "\n",
    "        state, action = next_state, next_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f9335d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Learned SARSA Q-table (normalized):\n",
      "\n",
      "       0      1      2      3      4      5      6\n",
      "0   0.00   0.00   0.00  63.99   0.00   0.00    0.0\n",
      "1   0.00   0.00  63.07   0.00   0.00   0.00    0.0\n",
      "2   0.00  28.53   0.00  19.48   0.00  79.96    0.0\n",
      "3  50.84   0.00  49.06   0.00  80.00   0.00    0.0\n",
      "4   0.00   0.00   0.00  44.54   0.00  80.00  100.0\n",
      "5   0.00   0.00  32.49   0.00  44.59   0.00  100.0\n",
      "6   0.00   0.00   0.00   0.00   0.00   0.00    0.0\n"
     ]
    }
   ],
   "source": [
    "# Normalize Q-table for easier interpretation\n",
    "Q_normalized = Q / Q.max() * 100\n",
    "\n",
    "# Print normalized Q-table\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.precision\", 2)\n",
    "print(\"\\nâœ… Learned SARSA Q-table (normalized):\\n\")\n",
    "print(pd.DataFrame(Q_normalized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "60196a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Œ Optimal policy from each state:\n",
      "From state 0 âœ go to state 3\n",
      "From state 1 âœ go to state 2\n",
      "From state 2 âœ go to state 5\n",
      "From state 3 âœ go to state 4\n",
      "From state 4 âœ go to state 6\n",
      "From state 5 âœ go to state 6\n",
      "From state 6 âœ go to state 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Print optimal policy derived from Q-table\n",
    "print(\"\\nğŸ“Œ Optimal policy from each state:\")\n",
    "for s in range(n_states):\n",
    "    best_a = np.argmax(Q[s])\n",
    "    print(f\"From state {s} âœ go to state {best_a}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28abd703",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62744644",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
